# Определение

Обратное распространение вычисляет [градиент](https://en.wikipedia.org/wiki/Gradient "Gradient") функции потерь по отношению к весам сети для одного примера ввода-вывода и делает это эффективно, в отличие от прямого вычисления градиента относительно каждого веса в отдельности. Эта эффективность делает возможным использование градиентных методов для обучения многослойных сетей и обновление весов для минимизации потерь. Обычно используется градиентный спуск или стохастический градиентный спуск. 

### Функция потерь
По другому *функция ошибки* — это функция, которая отображает значения одной или нескольких переменных в вещественное число. Для обратного распространения функция потерь вычисляет разницу между выходом сети и его ожидаемым выходом после того, как обучающий пример распространился по сети.

#### Пример реализации
-   [Статья «A Step by Step Backpropagation Example» от Matt Mazur](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
-   [Код по статье (реализация нейросети)](https://drive.google.com/file/d/16hiZefHVkIb8Cvhf3BbBXsOOV9hob7eo/view?usp=sharing)
-   [Обсуждение примера реализации](https://www.facebook.com/konstantin.berlinskii/posts/1463959563761486)

#### Режимы реализации
Существует два режима реализации метода обратного распространения ошибки:
1. Стохастического градиентного спуска
2. Пакетного

Для пакетного градиентного спуска функция потерь вычисляется для всех образцов вместе взятых после окончания эпохи, и потом вводятся поправки весовых коэффициентов нейрона в соответствии с методом обратного распространения ошибки.

Стохастический метод немедленно после вычисления выхода сети на одном образце вводит поправки в весовые коэффициенты.

Пакетный метод более быстрый и стабильный, но он имеет тенденцию останавливаться и застревать в локальных минимумах. Поэтому для выхода из локальных минимумов нужно использовать особые приёмы.

Стохастический метод медленнее, но от того, что он не осуществляет точного градиентного спуска, а вносит «шумы», используя недовычисленный градиент, он способен выходить из локальных минимумов и может привести к лучшему результату.

В виде компромисса рекомендуют также применять мини-пакеты, когда поправка искомых весов осуществляется после обработки нескольких образцов (мини-пакета), то есть, реже чем при стохастическом спуске, но чаще чем при пакетном.
